{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc945eb2-0b34-47f7-82d5-541a5f17e9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/12 18:54:50 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Set Dectron2-----------------------------------------------#\n",
    "# refer to: https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=7d3KxiHO_0gb\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import torch, detectron2\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "# from google.colab.patches import cv2_imshow # this is for Google colab, we use cv2 saveimg instead.\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from mono_infer import parse_args, init_mono, test_simple, get_paths\n",
    "\n",
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0f8878-8abc-4540-a939-d8ab733d1ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--model_name {mono_640x192,stereo_640x192,mono+stereo_640x192,mono_no_pt_640x192,stereo_no_pt_640x192,mono+stereo_no_pt_640x192,mono_1024x320,stereo_1024x320,mono+stereo_1024x320}]\n",
      "                             [--ext EXT] [--no_cuda] [--pred_metric_depth]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jovyan/.local/share/jupyter/runtime/kernel-439491d6-ca52-4e02-98b2-937f73bb9f06.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Set Mono2-----------------------------------------------#\n",
    "#refer to https://github.com/nianticlabs/monodepth2(I do not think it is a good model)\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import argparse\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import networks\n",
    "from layers import disp_to_depth\n",
    "from utils import download_model_if_doesnt_exist\n",
    "from evaluate_depth import STEREO_SCALE_FACTOR\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "args = parse_args()\n",
    "# FINDING INPUT IMAGES\n",
    "encoder, depth_decoder, feed_height, feed_width = init_mono(args, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09e850-24d0-44e4-8685-6af13bb5935b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c670ca9-5eac-4b79-8792-742c57f352d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nomo",
   "language": "python",
   "name": "nomo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
